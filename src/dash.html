import  { useState, useEffect } from 'react';

const questionsList = [
  { id: 1, text: "Let’s talk about your hometown. Where is your hometown?" },
  { id: 2, text: "What do you like about it?" },
  { id: 3, text: "What do you not like about it?" },
  { id: 4, text: "How important is your hometown to you?" },
  { id: 5, text: "Do you think you will continue to live in your hometown?" },
  { id: 6, text: "Let’s move on to talk about accommodation. Tell me about the kind of accommodation you live in?" },
  { id: 7, text: "Does the place you live in have many amenities?" },
  { id: 8, text: "Is there anything you would like to change about the place you live in?" },
  { id: 9, text: "Do you plan to live there for a long time?" },
  // Additional questions...
].map(q => ({ ...q, answer: "", played: false }));


const SpeechToText = () => {
  const [questions, setQuestions] = useState(questionsList);
  const [currentQuestionIndex, setCurrentQuestionIndex] = useState(0);
  const [isRecording, setIsRecording] = useState(false);
  const [testStarted, setTestStarted] = useState(false);
  const [recognitionActive, setRecognitionActive] = useState(false);
  const [userName, setUserName] = useState("");
  const [recordingName, setRecordingName] = useState(false);


  useEffect(() => {
    if ('webkitSpeechRecognition' in window) {
      const recognition = new window.webkitSpeechRecognition();
      recognition.lang = 'en-US';
      recognition.continuous = false; // Capture a single result
      recognition.interimResults = true;

      recognition.onresult = (event) => {
        let finalTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          }
        }
        const newQuestions = questions.map((q, idx) => 
          idx === currentQuestionIndex ? { ...q, answer: finalTranscript } : q
        );
        setQuestions(newQuestions);
      };

      recognition.onend = () => {
        setIsRecording(false);
        setRecognitionActive(false);
        if (testStarted && currentQuestionIndex < questions.length - 1) {
          setCurrentQuestionIndex((prevIndex) => prevIndex + 1);
        } else {
          setTestStarted(false);
        }
      };

      window.recognition = recognition;
    } else {
      alert("Your browser does not support the Web Speech API. Please use a supported browser.");
    }
  }, [testStarted, currentQuestionIndex, questions]);

  useEffect(() => {
    if (testStarted && currentQuestionIndex < questions.length) {
      const questionText = questions[currentQuestionIndex].text;
      speakQuestion(questionText);
    }
  }, [testStarted, currentQuestionIndex]);


  const speakIntroduction = () => {
    const msg = new SpeechSynthesisUtterance();
    msg.text = "Welcome to the Speaking section of the IELTS examination. My name is Brandon Coli and I will be your examiner for this part of the test and for your information, this test will be recorded. May I know your full name please?";
    msg.onend = () => {
      setRecordingName(true); // Set state to indicate recording name
      startRecordingName(); // Start recording user's name
    };
    window.speechSynthesis.speak(msg);
  };

  const startRecordingName = () => {
    const recognition = new window.webkitSpeechRecognition();
    recognition.lang = 'en-US';
    recognition.continuous = false; // Capture a single result
    recognition.interimResults = true;
  
    recognition.onresult = (event) => {
      let finalTranscript = '';
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        if (event.results[i].isFinal) {
          finalTranscript += event.results[i][0].transcript;
        }
      }
      setUserName(finalTranscript); // Set the recorded user's name
      setRecordingName(false); // Recording name complete
      startTest(); // Start the test after recording name
    };
  
    recognition.onend = () => {
      setIsRecording(false);
      setRecognitionActive(false);
    };
  
    setIsRecording(true);
    setRecognitionActive(true);
    recognition.start();
  
    // Set a timeout to stop recording after 2 seconds
    setTimeout(() => {
      recognition.stop();
      setIsRecording(false);
      setRecognitionActive(false);
      if (!userName) {
        // If the user hasn't spoken their name within 2 seconds, inform and proceed
        console.log("User didn't speak their name within 2 seconds");
        startTest(); // Start the test
      }
    }, 5000);
  };
  

  const speakQuestion = (questionText) => {
    const msg = new SpeechSynthesisUtterance();
    msg.text = questionText;
    msg.onend = () => {
      if (testStarted) {
        startAnswerRecording();
      }
    };
    window.speechSynthesis.speak(msg);
  };

  const startAnswerRecording = () => {
    if (!recognitionActive) {
      setIsRecording(true);
      setRecognitionActive(true);
      const recognition = new window.webkitSpeechRecognition();
      recognition.lang = 'en-US';
      recognition.continuous = true; // Keep recognition active for pauses in speech
      recognition.interimResults = true; // Capture interim results
  
      recognition.start();
  
      // Stop recognition after exactly 10 seconds
      setTimeout(() => {
        recognition.stop();
        setIsRecording(false);
        setRecognitionActive(false);
        if (testStarted && currentQuestionIndex < questions.length - 1) {
          setCurrentQuestionIndex((prevIndex) => prevIndex + 1);
        } else {
          setTestStarted(false);
        }
      }, 2000); // 10 seconds
  
      recognition.onresult = (event) => {
        let interimTranscript = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          if (event.results[i].isFinal) {
            const newQuestions = questions.map((q, idx) => 
              idx === currentQuestionIndex ? { ...q, answer: event.results[i][0].transcript } : q
            );
            setQuestions(newQuestions);
            console.log("Question:", newQuestions[currentQuestionIndex].text);
            console.log("Answer:", newQuestions[currentQuestionIndex].answer);
          } else {
            interimTranscript += event.results[i][0].transcript;
          }
        }
        // Optional: Use interimTranscript for real-time display of speech being recognized
      };
  
      recognition.onend = () => {
        setIsRecording(false);
        setRecognitionActive(false);
      };
    }
  };
  


const handleStartTest = () => {
  setTestStarted(true); // Set this here to prevent multiple button presses
  speakIntroduction(); // Start with the introduction
  startTest(); // Start the test
};


const startTest = () => {
  setCurrentQuestionIndex(0); // Ensure the first question is ready to be spoken after introduction
  setQuestions(questionsList.map(q => ({ ...q, answer: "", played: false }))); // Reset answers and played status
};




  const handleCloseTest = () => {
    window.speechSynthesis.cancel();
    if (window.recognition && recognitionActive) {
      window.recognition.stop();
    }
    setIsRecording(false);
    setTestStarted(false);
    setCurrentQuestionIndex(0);
    setQuestions(questionsList.map(q => ({ ...q, answer: "", played: false })));
  };

  // Your existing functions

  return (
    <div className="App">
      <div className="min-h-screen bg-gray-100 flex flex-col justify-center items-center">
        <h1 className="text-3xl font-bold mb-4">IELTS Speaking Test</h1>
        {!testStarted && (
          <button onClick={handleStartTest} className="bg-green-500 hover:bg-green-600 text-white font-bold py-2 px-4 rounded mt-4 mb-8">
            Let's start the test
          </button>
        )}
        
        {testStarted && (
          <div key={questions[currentQuestionIndex].id} className="p-4 max-w-md mx-auto bg-white rounded-xl shadow-md overflow-hidden md:max-w-2xl my-4">
            <div className="md:flex">
              <div className="p-8">
                
                <div className="uppercase tracking-wide text-sm text-indigo-500 font-semibold">{questions[currentQuestionIndex].text}</div>
                <p className="mt-2 text-gray-500">{questions[currentQuestionIndex].answer}</p>
                {isRecording ? <p>Recording...</p> : null}
              </div>
            </div>
          </div>
        )}
        {testStarted && (
          <button onClick={handleCloseTest} className="bg-red-500 hover:bg-red-600 text-white font-bold py-2 px-4 rounded mt-4 mb-8">
            Close the Test
          </button>
        )}
      </div>
    </div>
  );
};

export default SpeechToText;
 Analyse the code intelligently  and fix the name recording problem. its not waiting for 5 seconds to record the name but proceeds with the test without waiting for the name to be recorded. 